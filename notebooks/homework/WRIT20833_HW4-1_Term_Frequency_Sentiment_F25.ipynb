{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G717zEif3X-"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/homework/WRIT20833_HW4-1_Term_Frequency_Sentiment_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz1wJpA4f3X_"
      },
      "source": [
        "# HW4-1: Term Frequency and Sentiment Analysis\n",
        "**Due: October 5th**  \n",
        "**Intro to Coding in the Humanities - Midterm Assignment Part 1**\n",
        "\n",
        "**Student Name:** _Replace with your name_\n",
        "\n",
        "**Upload:** `LASTNAME_HW4-1_F25.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "## Assignment Overview\n",
        "\n",
        "This assignment marks your first extensive analysis of a text-heavy dataset of your choosing. The goal is not just to run analytical tools, but to experience the process of moving from assumptions to data-driven insights. You'll practice forming hypotheses, testing them against actual data, and allowing your understanding to evolve based on what you discover.\n",
        "\n",
        "In our digital age, we're constantly exposed to two very different paths to forming opinions and understanding the world. One path leads from genuine phenomena through careful data collection and analysis toward real insight, knowledge, and wisdom. The other path starts with social media impressions and moves through cherry-picked evidence and narrative scaffolding toward rigid ideological positions. Through this assignment, you'll experience the first path‚Äîthe harder but more rewarding journey of letting data challenge and refine your thinking.\n",
        "\n",
        "**Important Philosophy**: Being \"wrong\" in your initial predictions is not a failure‚Äîit's a sign of genuine learning and intellectual growth. The best insights often come from discovering that reality differs from our assumptions. When your data surprises you, that's when real learning happens.\n",
        "\n",
        "**Collaboration Policy**: You may work together with classmates to solve technical challenges and discuss methods, but each student must choose their own unique dataset and complete their own analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEGC_B5_f3X_"
      },
      "source": [
        "## Step 1: Dataset Loading and Initial Exploration\n",
        "*Moving from Scraped Data to Analysis-Ready Dataset*\n",
        "\n",
        "### Loading Your Cultural Dataset\n",
        "\n",
        "You'll use the cultural dataset you collected using the Instant Data Scraper in our previous lesson. This should be a CSV file with rich text content suitable for both term frequency and sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udR5pcQPf3YA"
      },
      "outputs": [],
      "source": [
        "# Setup: Import all libraries we'll need\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Display settings for better output\n",
        "pd.options.display.max_rows = 20\n",
        "pd.options.display.max_columns = 10\n",
        "\n",
        "print(\"üìö Libraries imported - ready for analysis!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqslu6Yaf3YA"
      },
      "outputs": [],
      "source": [
        "# Load your scraped cultural data\n",
        "# Replace 'your_filename.csv' with your actual file name\n",
        "\n",
        "# If you're in Google Colab, upload your file first:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Load your data (fill in your filename)\n",
        "df = pd.read_csv('_____')  # Replace with your CSV filename\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Dataset contains {len(df)} items\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy-C5Lz-f3YA"
      },
      "source": [
        "### Technical Checkpoint 1: Data Loading Reality Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLE9-eM2f3YA"
      },
      "outputs": [],
      "source": [
        "# Checkpoint 1: Verify your data loaded correctly\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Replace 'YOUR_TEXT_COLUMN' with your actual text column name\n",
        "text_column = '_____'  # Fill in your main text column name\n",
        "\n",
        "print(f\"Text column has {df[text_column].isna().sum()} missing values\")\n",
        "print(f\"Sample text: {df[text_column].iloc[0][:200]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6wqnszif3YB"
      },
      "outputs": [],
      "source": [
        "# Explore your dataset structure\n",
        "print(\"=== DATASET EXPLORATION ===\")\n",
        "print(f\"Total entries: {len(df)}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko2SKiWOf3YB"
      },
      "source": [
        "### Initial Predictions\n",
        "\n",
        "Before analyzing your data, record your predictions. This is like forming \"first impressions\" from HW1, but now for both word frequency AND sentiment patterns.\n",
        "\n",
        "**Examples to model your thinking:**\n",
        "- *Restaurant reviews*: \"I predict negative reviews will focus on 'slow,' 'cold,' and 'rude' while positive reviews will emphasize 'fresh,' 'friendly,' and 'delicious.' I expect about 60% positive sentiment overall.\"\n",
        "- *Museum reviews*: \"I predict frequent words like 'interesting,' 'beautiful,' and 'educational' with mostly positive sentiment, but some complaints about 'crowded' or 'expensive.'\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fb9yJUTf3YB"
      },
      "source": [
        "### üìù My Initial Predictions:\n",
        "\n",
        "**Dataset Description:** *(What did you scrape and why did you choose it?)*\n",
        "\n",
        "\n",
        "**Term Frequency Predictions:** *(What specific words do you expect to appear most frequently?)*\n",
        "\n",
        "\n",
        "**Sentiment Predictions:** *(What sentiment patterns do you expect? Mostly positive/negative/mixed? Any specific emotional themes?)*\n",
        "\n",
        "\n",
        "**Most Confident Prediction:** *(What are you most sure about?)*\n",
        "\n",
        "\n",
        "**Biggest Question:** *(What are you most curious to discover?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpEXWK8xf3YB"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 2: Text Preparation for Analysis\n",
        "*Building on HW1 Text Processing Skills*\n",
        "\n",
        "Just like in HW1, we need to prepare our text for analysis. But this time, we're preparing for TWO types of analysis: term frequency (like HW1) and sentiment analysis (new!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCMBtQb-f3YB"
      },
      "outputs": [],
      "source": [
        "# Enhanced stopwords list (building on HW1)\n",
        "stopwords = [\n",
        "    # Basic stopwords from HW1\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
        "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "    \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
        "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\",\n",
        "    \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
        "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
        "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
        "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
        "    \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
        "    \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
        "    \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"ve\", \"ll\", \"amp\",\n",
        "\n",
        "    # Additional common words for cultural data\n",
        "    \"also\", \"would\", \"could\", \"get\", \"go\", \"one\", \"two\", \"see\", \"time\", \"way\", \"may\",\n",
        "    \"said\", \"say\", \"new\", \"first\", \"last\", \"long\", \"little\", \"much\", \"well\", \"still\"\n",
        "]\n",
        "\n",
        "print(f\"‚úÖ Stopwords list loaded: {len(stopwords)} words to filter out\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ibDd59Yf3YC"
      },
      "outputs": [],
      "source": [
        "# Text processing functions (building on HW1 approach)\n",
        "def split_into_words(text):\n",
        "    \"\"\"Split text into words (same approach as HW1)\"\"\"\n",
        "    if pd.isna(text):  # Handle missing text\n",
        "        return []\n",
        "    lowercase_text = str(text).lower()\n",
        "    # Split text into words by looking for places where letters/numbers are not present\n",
        "    split_words = re.split(\"\\\\W+\", lowercase_text)\n",
        "    return [word for word in split_words if word]  # Remove empty strings\n",
        "\n",
        "def clean_for_sentiment(text):\n",
        "    \"\"\"Clean text for sentiment analysis (keep punctuation!)\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # Keep punctuation and capitalization - VADER needs them!\n",
        "    return str(text).strip()\n",
        "\n",
        "print(\"‚úÖ Text processing functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvFpQmOyf3YC"
      },
      "outputs": [],
      "source": [
        "# Apply text cleaning to your dataset\n",
        "# Replace 'text_column' with your actual column name\n",
        "\n",
        "# Create cleaned text for sentiment analysis (keep punctuation)\n",
        "df['clean_text_sentiment'] = df[text_column].apply(clean_for_sentiment)\n",
        "\n",
        "# Create word lists for term frequency analysis (like HW1)\n",
        "df['words'] = df[text_column].apply(split_into_words)\n",
        "df['meaningful_words'] = df['words'].apply(lambda word_list: [word for word in word_list if word not in stopwords])\n",
        "\n",
        "print(\"‚úÖ Text cleaning complete\")\n",
        "print(f\"Sample cleaned text for sentiment: {df['clean_text_sentiment'].iloc[0][:100]}...\")\n",
        "print(f\"Sample meaningful words: {df['meaningful_words'].iloc[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYN-q6lf3YC"
      },
      "source": [
        "### Technical Checkpoint 2: Text Cleaning Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA5VjSOnf3YC"
      },
      "outputs": [],
      "source": [
        "# Checkpoint 2: Check your cleaning worked\n",
        "sample_text = df[text_column].iloc[0]\n",
        "print(f\"Original: {sample_text[:100]}\")\n",
        "print(f\"For sentiment: {df['clean_text_sentiment'].iloc[0][:100]}\")\n",
        "print(f\"Meaningful words: {df['meaningful_words'].iloc[0][:15]}\")\n",
        "# Should show: sentiment text keeps punctuation, meaningful words exclude stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQlpS9B4f3YC"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Term Frequency Analysis\n",
        "*Building on HW1 Word Counting Skills*\n",
        "\n",
        "Now let's analyze which words appear most frequently in your cultural dataset, just like you did in HW1 but with more sophisticated text processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzceihOsf3YC"
      },
      "outputs": [],
      "source": [
        "# Combine all meaningful words from your dataset (like HW1)\n",
        "all_meaningful_words = []\n",
        "for word_list in df['meaningful_words']:\n",
        "    all_meaningful_words.extend(word_list)\n",
        "\n",
        "# Count word frequencies (same as HW1)\n",
        "word_frequency = Counter(all_meaningful_words)\n",
        "top_words = word_frequency.most_common(20)  # Get top 20 words\n",
        "\n",
        "print(\"üî§ TERM FREQUENCY ANALYSIS RESULTS\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total words analyzed: {len(all_meaningful_words):,}\")\n",
        "print(f\"Unique words found: {len(word_frequency):,}\")\n",
        "print(f\"\\nTop 20 most frequent words:\")\n",
        "\n",
        "for i, (word, count) in enumerate(top_words, 1):\n",
        "    print(f\"{i:2d}. {word:<15} ({count:,} times)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRUZ-tR_f3YC"
      },
      "outputs": [],
      "source": [
        "# Create visualization of top words\n",
        "words = [word for word, count in top_words]\n",
        "counts = [count for word, count in top_words]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(range(len(words)), counts, color='skyblue')\n",
        "plt.yticks(range(len(words)), words)\n",
        "plt.xlabel('Frequency')\n",
        "plt.title(f'Top {len(words)} Most Frequent Words in Your Cultural Dataset')\n",
        "plt.gca().invert_yaxis()  # Put highest frequency at top\n",
        "\n",
        "# Add count labels on bars\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(count + max(counts)*0.01, i, str(count), va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Term frequency visualization complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S-dSFtxf3YC"
      },
      "source": [
        "### üìù Term Frequency Reflection:\n",
        "\n",
        "**Prediction Check:** *(Which of your predicted words actually appeared in the top frequencies? What surprised you most?)*\n",
        "\n",
        "\n",
        "**New Discoveries:** *(What words appeared that you didn't expect? What do they tell you about your dataset?)*\n",
        "\n",
        "\n",
        "**Questions for Sentiment Analysis:** *(Based on these word frequencies, what questions do you now want to explore with sentiment analysis?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29_Wri7f3YC"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 4: Sentiment Analysis with VADER\n",
        "*Building on VADER CodeAlong Skills*\n",
        "\n",
        "Now let's analyze the emotional tone of your cultural texts using VADER, focusing on the compound sentiment scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loxs68CTf3YC"
      },
      "outputs": [],
      "source": [
        "# Install and import VADER\n",
        "!pip install vaderSentiment\n",
        "\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "print(\"‚úÖ VADER installed and ready for sentiment analysis!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO8ojQi8f3YD"
      },
      "source": [
        "### Technical Checkpoint 3: VADER Installation/Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtaWl8YTf3YD"
      },
      "outputs": [],
      "source": [
        "# Checkpoint 3: VADER setup\n",
        "test_text = \"This assignment is surprisingly interesting!\"\n",
        "test_result = analyzer.polarity_scores(test_text)\n",
        "print(f\"Test text: {test_text}\")\n",
        "print(f\"VADER result: {test_result}\")\n",
        "# Should see: {'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.6588} (or similar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF_Z0WJwf3YD"
      },
      "outputs": [],
      "source": [
        "# Apply sentiment analysis to your entire dataset\n",
        "def get_sentiment_score(text):\n",
        "    \"\"\"Get compound sentiment score for a text\"\"\"\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores['compound']\n",
        "\n",
        "# Apply sentiment analysis to your entire dataset\n",
        "df['sentiment_score'] = df['clean_text_sentiment'].apply(_____) # Fill in: what function?\n",
        "\n",
        "print(\"‚úÖ Sentiment analysis complete for entire dataset!\")\n",
        "print(f\"\\nSentiment score range: {df['sentiment_score'].min():.3f} to {df['sentiment_score'].max():.3f}\")\n",
        "print(f\"Average sentiment: {df['sentiment_score'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK9xr3oqf3YD"
      },
      "outputs": [],
      "source": [
        "# Analyze sentiment patterns in your data\n",
        "print(\"üé≠ SENTIMENT ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"Total entries analyzed: {len(df)}\")\n",
        "print(f\"Average sentiment: {df['sentiment_score'].mean():.3f}\")\n",
        "print(f\"Most positive entry: {df['sentiment_score'].max():.3f}\")\n",
        "print(f\"Most negative entry: {df['sentiment_score'].min():.3f}\")\n",
        "print(f\"Standard deviation: {df['sentiment_score'].std():.3f}\")\n",
        "\n",
        "# Categorize sentiments\n",
        "positive = len(df[df['sentiment_score'] > 0.1])\n",
        "neutral = len(df[(df['sentiment_score'] >= -0.1) & (df['sentiment_score'] <= 0.1)])\n",
        "negative = len(df[df['sentiment_score'] < -0.1])\n",
        "\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(f\"Positive (>0.1): {positive} ({positive/len(df)*100:.1f}%)\")\n",
        "print(f\"Neutral (-0.1 to 0.1): {neutral} ({neutral/len(df)*100:.1f}%)\")\n",
        "print(f\"Negative (<-0.1): {negative} ({negative/len(df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH2Mx0scf3YD"
      },
      "outputs": [],
      "source": [
        "# Find most positive and negative examples\n",
        "most_positive = df.loc[df['sentiment_score'].idxmax()]\n",
        "most_negative = df.loc[df['sentiment_score'].idxmin()]\n",
        "\n",
        "print(f\"üìà MOST POSITIVE ENTRY (score: {most_positive['sentiment_score']:.3f}):\")\n",
        "print(f\"Text: {most_positive['clean_text_sentiment'][:200]}...\")\n",
        "print()\n",
        "print(f\"üìâ MOST NEGATIVE ENTRY (score: {most_negative['sentiment_score']:.3f}):\")\n",
        "print(f\"Text: {most_negative['clean_text_sentiment'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ykCpliwf3YD"
      },
      "outputs": [],
      "source": [
        "# Create sentiment visualizations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Sentiment distribution histogram\n",
        "axes[0].hist(df['sentiment_score'], bins=20, color='lightblue', alpha=0.7, edgecolor='black')\n",
        "axes[0].set_title('Distribution of Sentiment Scores')\n",
        "axes[0].set_xlabel('Sentiment Score (-1 to 1)')\n",
        "axes[0].set_ylabel('Number of Entries')\n",
        "axes[0].axvline(0, color='red', linestyle='--', alpha=0.7, label='Neutral')\n",
        "axes[0].axvline(df['sentiment_score'].mean(), color='green', linestyle='--', alpha=0.7, label='Average')\n",
        "axes[0].legend()\n",
        "\n",
        "# Sentiment categories pie chart\n",
        "sentiment_counts = [positive, neutral, negative]\n",
        "sentiment_labels = [f'Positive\\n({positive})', f'Neutral\\n({neutral})', f'Negative\\n({negative})']\n",
        "colors = ['lightgreen', 'lightgray', 'lightcoral']\n",
        "\n",
        "axes[1].pie(sentiment_counts, labels=sentiment_labels, colors=colors, autopct='%1.1f%%')\n",
        "axes[1].set_title('Sentiment Category Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Sentiment visualizations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvH-zlqBf3YD"
      },
      "source": [
        "### Human vs. Automated Sentiment Check\n",
        "\n",
        "Let's test VADER's accuracy by comparing it to your human judgment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8IEj3tMf3YD"
      },
      "outputs": [],
      "source": [
        "# Sample 5 entries for human vs. VADER comparison\n",
        "sample_entries = df.sample(5)\n",
        "\n",
        "print(\"üß† HUMAN vs. VADER SENTIMENT COMPARISON\")\n",
        "print(\"=\" * 45)\n",
        "print(\"Read each text below and judge its sentiment, then compare to VADER's score:\\n\")\n",
        "\n",
        "for i, (idx, row) in enumerate(sample_entries.iterrows(), 1):\n",
        "    print(f\"Text {i}:\")\n",
        "    print(f\"'{row['clean_text_sentiment'][:150]}...'\")\n",
        "    print(f\"VADER Score: {row['sentiment_score']:.3f}\")\n",
        "    print(f\"Your Human Judgment: _____ (Positive/Neutral/Negative)\")\n",
        "    print(f\"Agreement? _____ (Yes/No - explain any differences)\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DMMesYAf3YD"
      },
      "source": [
        "### üìù Sentiment Analysis Reflection:\n",
        "\n",
        "**Sentiment Patterns:** *(What did sentiment analysis reveal? How did these results compare to your expectations? Were there more positive, negative, or neutral entries than you predicted?)*\n",
        "\n",
        "\n",
        "**VADER Accuracy:** *(Based on your human vs. VADER comparison above, where do you agree or disagree with the tool's assessment? What types of text does VADER handle well or poorly in your dataset?)*\n",
        "\n",
        "\n",
        "**Unexpected Discoveries:** *(What surprised you most about the sentiment patterns in your data?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9HKlSJbf3YD"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 5: Integration and Critical Analysis\n",
        "*Connecting Term Frequency and Sentiment Insights*\n",
        "\n",
        "Now let's explore how your term frequency and sentiment findings work together to reveal deeper patterns in your cultural data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9hrBmKff3YD"
      },
      "outputs": [],
      "source": [
        "# Analyze word frequency in positive vs. negative entries\n",
        "positive_entries = df[df['sentiment_score'] > 0.1]\n",
        "negative_entries = df[df['sentiment_score'] < -0.1]\n",
        "\n",
        "print(\"üîç COMPARING WORDS IN POSITIVE vs. NEGATIVE ENTRIES\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Get top words from positive entries\n",
        "positive_words = []\n",
        "for word_list in positive_entries['meaningful_words']:\n",
        "    positive_words.extend(word_list)\n",
        "positive_freq = Counter(positive_words)\n",
        "\n",
        "# Get top words from negative entries\n",
        "negative_words = []\n",
        "for word_list in negative_entries['meaningful_words']:\n",
        "    negative_words.extend(word_list)\n",
        "negative_freq = Counter(negative_words)\n",
        "\n",
        "print(f\"Top 10 words in POSITIVE entries ({len(positive_entries)} entries):\")\n",
        "for word, count in positive_freq.most_common(10):\n",
        "    print(f\"  {word:<15} ({count} times)\")\n",
        "\n",
        "print(f\"\\nTop 10 words in NEGATIVE entries ({len(negative_entries)} entries):\")\n",
        "for word, count in negative_freq.most_common(10):\n",
        "    print(f\"  {word:<15} ({count} times)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4OQ7z_9f3YD"
      },
      "outputs": [],
      "source": [
        "# Create comparison visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Positive words\n",
        "pos_words = [word for word, count in positive_freq.most_common(10)]\n",
        "pos_counts = [count for word, count in positive_freq.most_common(10)]\n",
        "\n",
        "axes[0].barh(range(len(pos_words)), pos_counts, color='lightgreen')\n",
        "axes[0].set_yticks(range(len(pos_words)))\n",
        "axes[0].set_yticklabels(pos_words)\n",
        "axes[0].set_title(f'Top Words in Positive Entries (n={len(positive_entries)})')\n",
        "axes[0].set_xlabel('Frequency')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Negative words\n",
        "neg_words = [word for word, count in negative_freq.most_common(10)]\n",
        "neg_counts = [count for word, count in negative_freq.most_common(10)]\n",
        "\n",
        "axes[1].barh(range(len(neg_words)), neg_counts, color='lightcoral')\n",
        "axes[1].set_yticks(range(len(neg_words)))\n",
        "axes[1].set_yticklabels(neg_words)\n",
        "axes[1].set_title(f'Top Words in Negative Entries (n={len(negative_entries)})')\n",
        "axes[1].set_xlabel('Frequency')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p62hndcBf3YD"
      },
      "source": [
        "### Bridge to Part 2 (HW4-2)\n",
        "\n",
        "Based on your term frequency and sentiment findings, make a prediction about what deeper topics or themes topic modeling might reveal in your data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpuzHP57f3YD"
      },
      "source": [
        "### üìù Integration Analysis and HW4-2 Prediction:\n",
        "\n",
        "**Combined Insights:** *(How do your term frequency and sentiment findings work together? What patterns emerge when you consider both word frequency AND emotional tone?)*\n",
        "\n",
        "\n",
        "**Most Revealing Analysis:** *(Which analytical method (term frequency or sentiment analysis) was most revealing for your dataset? What did you learn about what these tools can and cannot tell you?)*\n",
        "\n",
        "\n",
        "**Topic Modeling Prediction:** *(Based on your term frequency and sentiment findings, what deeper topics or themes do you predict topic modeling will reveal in your data? What specific topics do you expect to emerge?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6XgHOZRf3YE"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 6: Final Reflection\n",
        "*From Assumptions to Data-Driven Insights*\n",
        "\n",
        "This final reflection is the heart of the assignment. Think deeply about how your understanding evolved through this analysis process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXJyPV-df3YE"
      },
      "source": [
        "### üìù Final Reflection (3-4 paragraphs):\n",
        "\n",
        "Address these questions in your reflection:\n",
        "\n",
        "**Evolution of Understanding:** *How did your understanding change from your initial predictions through term frequency, then sentiment analysis? Where were you most wrong, and what did those surprises teach you?*\n",
        "\n",
        "**Computational vs. Traditional Analysis:** *How does this data-driven approach compare to how you might traditionally analyze texts in humanities courses? Consider how this process helps you avoid the trap of cherry-picking evidence to support predetermined conclusions. What are the advantages and limitations of automated analysis?*\n",
        "\n",
        "**Personal Learning:** *What did this process teach you about forming and testing hypotheses? How has your relationship to evidence and conclusions evolved?*\n",
        "\n",
        "**Cultural Insights:** *What did you discover about the cultural phenomenon you studied? What questions does this analysis raise for further research?*\n",
        "\n",
        "---\n",
        "\n",
        "**Write your reflection here:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlFCJR7Yf3YE"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 7: Save Your Work and Prepare Deliverables\n",
        "\n",
        "Make sure to save all your work and prepare the required files for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4MZRbJOf3YM"
      },
      "outputs": [],
      "source": [
        "# Save your cleaned dataset\n",
        "output_filename = 'LASTNAME_HW4-1_cleaned_data.csv'  # Replace LASTNAME with your actual last name\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"‚úÖ Cleaned dataset saved as: {output_filename}\")\n",
        "print(f\"\\nDataset summary:\")\n",
        "print(f\"- Total entries: {len(df)}\")\n",
        "print(f\"- Columns: {df.columns.tolist()}\")\n",
        "print(f\"- Average sentiment score: {df['sentiment_score'].mean():.3f}\")\n",
        "print(f\"- Most frequent word: '{top_words[0][0]}' ({top_words[0][1]} times)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0EQyHt3f3YM"
      },
      "source": [
        "### üìã Submission Checklist\n",
        "\n",
        "Before submitting, make sure you have:\n",
        "\n",
        "- [ ] **Completed all code cells** with your own data and analysis\n",
        "- [ ] **Filled in all prediction and reflection sections** with thoughtful responses\n",
        "- [ ] **Verified your visualizations** display correctly\n",
        "- [ ] **Saved your cleaned dataset** as a CSV file\n",
        "- [ ] **Named your files correctly**: `LASTNAME_HW4-1_F25.ipynb` and `LASTNAME_HW4-1_cleaned_data.csv`\n",
        "\n",
        "### üì§ What to Submit:\n",
        "\n",
        "Submit to your **Pre-Project Practice Files** dropbox:\n",
        "1. **Jupyter Notebook** (`LASTNAME_HW4-1_F25.ipynb`) with all code and markdown responses\n",
        "2. **Clean dataset CSV** (`LASTNAME_HW4-1_cleaned_data.csv`)\n",
        "3. **Visualization PNG files** (if not embedded in notebook)\n",
        "\n",
        "**Due Date**: October 5th\n",
        "\n",
        "---\n",
        "\n",
        "## Looking Ahead to HW4-2\n",
        "\n",
        "You've now completed the first part of your text analysis journey! In **HW4-2**, you'll use topic modeling to discover hidden themes in your dataset and reflect on your complete analytical process.\n",
        "\n",
        "**Keep your cleaned data and insights** - you'll build on this work in the next assignment. The predictions you made about topic modeling will be tested against actual results!\n",
        "\n",
        "Great work on completing your first comprehensive text analysis project! üéâ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}