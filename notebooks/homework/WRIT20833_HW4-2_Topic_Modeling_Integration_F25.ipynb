{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IZe-tMOUbnR"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TCU-DCDA/WRIT20833-2025/blob/main/notebooks/homework/WRIT20833_HW4-2_Topic_Modeling_Integration_F25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsqHa7nhUbnd"
      },
      "source": [
        "# HW4-2: Topic Modeling and Integration\n",
        "**Due: October 17th**  \n",
        "**Intro to Coding in the Humanities - Midterm Assignment Part 2**\n",
        "\n",
        "**Student Name:** _Replace with your name_\n",
        "\n",
        "**Upload:** `LASTNAME_HW4-2_F25.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "## Assignment Overview\n",
        "\n",
        "You'll now use topic modeling to uncover hidden patterns in your dataset and reflect on your complete analytical journey. This assignment builds directly on your HW4-1 work.\n",
        "\n",
        "**Remember**: The best insights come from being surprised by your data. If your topic modeling results differ from your predictions, that's a sign of genuine learning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkDKfHrlUbng"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Reconnecting with Your Predictions\n",
        "*Before You Begin*\n",
        "\n",
        "### Review Your HW4-1 Work\n",
        "\n",
        "Before running any new code, take time to review what you've already learned:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkf9yH_pUbni"
      },
      "source": [
        "### üìù Reflection: Your Predictions from HW4-1\n",
        "\n",
        "**Your Topic Modeling Predictions from HW4-1 Step 5:**\n",
        "\n",
        "*(Copy your topic modeling predictions from HW4-1. What specific topics did you predict would emerge?)*\n",
        "\n",
        "1. Topic 1:\n",
        "\n",
        "2. Topic 2:\n",
        "\n",
        "3. Topic 3:\n",
        "\n",
        "**Current State of Knowledge:**\n",
        "\n",
        "*(After completing HW4-1's term frequency and sentiment analysis, how has your understanding evolved? What do you now think topic modeling might reveal?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6QnQKfmUbnk"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 2: Loading Your HW4-1 Data\n",
        "*Picking Up Where You Left Off*\n",
        "\n",
        "You'll continue working with the same dataset from HW4-1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IHJYKJrLUbnm",
        "outputId": "60e1c566-3a9c-4944-c230-bf62b7d27c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies (this takes ~30 seconds)...\n",
            "============================================================\n",
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: scipy 1.16.3\n",
            "Uninstalling scipy-1.16.3:\n",
            "  Successfully uninstalled scipy-1.16.3\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "============================================================\n",
            "‚úÖ Installation complete!\n",
            "============================================================\n",
            "\n",
            "üîÑ üîÑ üîÑ STOP! MANDATORY NEXT STEP üîÑ üîÑ üîÑ\n",
            "\n",
            "You MUST restart the runtime before continuing:\n",
            "   1. Click 'Runtime' in the menu bar above\n",
            "   2. Select 'Restart runtime'\n",
            "   3. When prompted, click 'Yes' to confirm\n",
            "   4. Then run the NEXT cell to import libraries\n",
            "\n",
            "‚ö†Ô∏è  Do NOT skip this step or you will get errors!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install Required Libraries\n",
        "# Run this cell ONCE, then follow the instructions below\n",
        "\n",
        "print(\"üì¶ Installing dependencies (this takes ~30 seconds)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Uninstall conflicting packages\n",
        "!pip uninstall -y numpy pandas scipy\n",
        "\n",
        "# Install compatible versions together\n",
        "!pip install -q numpy==1.26.4 pandas==2.2.2 scipy==1.13.1\n",
        "\n",
        "# Install gensim and nltk\n",
        "!pip install -q gensim==4.3.3 nltk\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüîÑ üîÑ üîÑ STOP! MANDATORY NEXT STEP üîÑ üîÑ üîÑ\")\n",
        "print(\"\\nYou MUST restart the runtime before continuing:\")\n",
        "print(\"   1. Click 'Runtime' in the menu bar above\")\n",
        "print(\"   2. Select 'Restart runtime'\")\n",
        "print(\"   3. When prompted, click 'Yes' to confirm\")\n",
        "print(\"   4. Then run the NEXT cell to import libraries\")\n",
        "print(\"\\n‚ö†Ô∏è  Do NOT skip this step or you will get errors!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import Libraries\n",
        "# Run this cell ONLY AFTER restarting runtime\n",
        "\n",
        "print(\"üìö Importing libraries...\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ All libraries loaded and ready!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Gensim version: {gensim.__version__}\")\n",
        "print(\"\\nüéâ You're ready to proceed with the assignment!\")"
      ],
      "metadata": {
        "id": "p0JktOmnUbnx",
        "outputId": "945bea72-6e16-4d2e-ab8e-3af386541b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Importing libraries...\n",
            "============================================================\n",
            "‚úÖ All libraries loaded and ready!\n",
            "============================================================\n",
            "NumPy version: 1.26.4\n",
            "Gensim version: 4.3.3\n",
            "\n",
            "üéâ You're ready to proceed with the assignment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS3nGtnaUbnz"
      },
      "outputs": [],
      "source": [
        "# Load your cleaned dataset from HW4-1\n",
        "# If you're in Google Colab, upload your file first:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# Load your HW4-1 cleaned data\n",
        "df = pd.read_csv('_____')  # Replace with your HW4-1 cleaned CSV filename\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"Dataset contains {len(df)} items\")\n",
        "print(f\"\\nColumns available: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru9sJq88Ubn1"
      },
      "outputs": [],
      "source": [
        "# Verify your data is ready\n",
        "print(\"üìä DATASET SUMMARY FROM HW4-1\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total entries: {len(df)}\")\n",
        "print(f\"Average sentiment score: {df['sentiment_score'].mean():.3f}\")\n",
        "print(f\"\\nFirst few entries:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPauukwpUbn4"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 3: Text Preprocessing for Topic Modeling\n",
        "*Different from HW4-1 Preprocessing!*\n",
        "\n",
        "Topic modeling requires more aggressive text cleaning than sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5mJz1zhUbn5"
      },
      "outputs": [],
      "source": [
        "# Enhanced stopwords list for topic modeling\n",
        "stopwords = [\n",
        "    # Basic English stopwords\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
        "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\",\n",
        "    \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
        "    \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\",\n",
        "    \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
        "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
        "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
        "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\",\n",
        "    \"how\", \"all\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
        "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
        "    \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"ve\", \"ll\", \"amp\",\n",
        "    \"also\", \"would\", \"could\", \"get\", \"go\", \"one\", \"two\", \"see\", \"time\", \"way\",\n",
        "    \"may\", \"said\", \"say\", \"new\", \"first\", \"last\", \"long\", \"little\", \"much\",\n",
        "    \"well\", \"still\", \"even\", \"back\", \"good\", \"many\", \"make\", \"made\", \"us\", \"really\"\n",
        "]\n",
        "\n",
        "# ADD YOUR OWN DOMAIN-SPECIFIC STOPWORDS HERE\n",
        "# Examples: for restaurant reviews, add \"restaurant\", \"food\", \"place\"\n",
        "#           for book reviews, add \"book\", \"story\", \"read\"\n",
        "custom_stopwords = [_____]  # Fill in words specific to your dataset\n",
        "\n",
        "stopwords.extend(custom_stopwords)\n",
        "\n",
        "print(f\"‚úÖ Stopwords list loaded: {len(stopwords)} words to filter out\")\n",
        "print(f\"Custom stopwords added: {custom_stopwords}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B04YEe7Ubn7"
      },
      "outputs": [],
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_for_topics(text):\n",
        "    \"\"\"\n",
        "    Aggressive text preprocessing for topic modeling:\n",
        "    - Lowercase\n",
        "    - Remove punctuation\n",
        "    - Remove stopwords\n",
        "    - Lemmatize (reduce to base form)\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove punctuation and split into words\n",
        "    words = re.findall(r'\\b[a-z]+\\b', text)\n",
        "\n",
        "    # Remove stopwords and short words (< 3 characters)\n",
        "    words = [word for word in words if word not in stopwords and len(word) >= 3]\n",
        "\n",
        "    # Lemmatize words (reduce to base form)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "print(\"‚úÖ Preprocessing function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y5K-p5ZUbn9"
      },
      "outputs": [],
      "source": [
        "# Test preprocessing on one text\n",
        "text_column = '_____'  # Fill in your text column name from HW4-1\n",
        "\n",
        "sample_text = df[text_column].iloc[0]\n",
        "processed = preprocess_for_topics(sample_text)\n",
        "\n",
        "print(\"Text Preprocessing Test:\")\n",
        "print(f\"Original: {sample_text[:150]}...\")\n",
        "print(f\"\\nProcessed words: {processed}\")\n",
        "print(f\"\\nNotice: lowercase, no punctuation, lemmatized, stopwords removed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzTlEDxSUbn_"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to entire dataset\n",
        "df['processed_for_topics'] = df[text_column].apply(preprocess_for_topics)\n",
        "\n",
        "print(\"‚úÖ Preprocessing complete!\")\n",
        "print(f\"\\nProcessed {len(df)} documents\")\n",
        "print(f\"\\nExample processed documents:\")\n",
        "for i in range(3):\n",
        "    print(f\"{i+1}. {df['processed_for_topics'].iloc[i][:10]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h-q74LMUboA"
      },
      "source": [
        "### Technical Checkpoint 1: Data Preparation\n",
        "\n",
        "Before topic modeling, verify your data is ready:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0EduC8fUboA"
      },
      "outputs": [],
      "source": [
        "# Checkpoint: Verify data is ready for topic modeling\n",
        "doc_lengths = [len(doc) for doc in df['processed_for_topics']]\n",
        "avg_length = np.mean(doc_lengths)\n",
        "all_words = [word for doc in df['processed_for_topics'] for word in doc]\n",
        "vocab_size = len(set(all_words))\n",
        "\n",
        "print(\"üìä DATA PREPARATION CHECK\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Number of documents: {len(df)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Average document length: {avg_length:.1f} words\")\n",
        "print(f\"Shortest document: {min(doc_lengths)} words\")\n",
        "print(f\"Longest document: {max(doc_lengths)} words\")\n",
        "\n",
        "if avg_length < 10:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Average document length is very short. Topic modeling may struggle.\")\n",
        "if vocab_size < 100:\n",
        "    print(\"\\n‚ö†Ô∏è WARNING: Vocabulary size is small. Consider reducing custom stopwords.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mptR0AkzUboB"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 4: Building Your Topic Model\n",
        "*Discovering Hidden Themes*\n",
        "\n",
        "Now we'll use Gensim LDA to discover topics in your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mBUZ3_SUboB"
      },
      "outputs": [],
      "source": [
        "# Create Gensim dictionary and corpus\n",
        "dictionary = corpora.Dictionary(df['processed_for_topics'])\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df['processed_for_topics']]\n",
        "\n",
        "print(\"üìñ Dictionary and corpus created!\")\n",
        "print(f\"Total unique words in dictionary: {len(dictionary)}\")\n",
        "print(f\"Total documents in corpus: {len(corpus)}\")\n",
        "print(f\"\\nExample word-to-ID mappings:\")\n",
        "for i, (word_id, word) in enumerate(list(dictionary.items())[:10]):\n",
        "    print(f\"  ID {word_id}: {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgCXfR0KUboC"
      },
      "source": [
        "### Experimenting with Number of Topics\n",
        "\n",
        "Start by trying different numbers of topics to find the most interpretable results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgL-GzSEUboC"
      },
      "outputs": [],
      "source": [
        "# Experiment: Try different numbers of topics\n",
        "def train_and_display_topics(corpus, dictionary, num_topics):\n",
        "    \"\"\"\n",
        "    Train an LDA model and display discovered topics\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"MODEL WITH {num_topics} TOPICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = LdaModel(\n",
        "        corpus=corpus,\n",
        "        id2word=dictionary,\n",
        "        num_topics=num_topics,\n",
        "        random_state=42,\n",
        "        passes=15,\n",
        "        alpha='auto',\n",
        "        eta='auto'\n",
        "    )\n",
        "\n",
        "    for idx in range(num_topics):\n",
        "        words = model.show_topic(idx, 10)\n",
        "        word_list = [word for word, prob in words]\n",
        "        print(f\"Topic {idx}: {', '.join(word_list)}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Try 3, 5, and 7 topics\n",
        "print(\"üß™ EXPERIMENTING WITH DIFFERENT NUMBERS OF TOPICS\")\n",
        "print(\"Watch how topics change as we increase the number...\\n\")\n",
        "\n",
        "model_3 = train_and_display_topics(corpus, dictionary, 3)\n",
        "model_5 = train_and_display_topics(corpus, dictionary, 5)\n",
        "model_7 = train_and_display_topics(corpus, dictionary, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR0c_5YFUboD"
      },
      "source": [
        "### üìù Choose Your Best Model\n",
        "\n",
        "**Based on the experiments above, which number of topics gives the most interpretable results?**\n",
        "\n",
        "*(Consider: Are the topics distinct? Do they make sense? Are there too few (too broad) or too many (too fragmented)?)*\n",
        "\n",
        "**My choice:** I will use _____ topics because:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTorJEbQUboE"
      },
      "outputs": [],
      "source": [
        "# Train your final model with your chosen number of topics\n",
        "num_topics = _____  # Fill in your chosen number (3, 5, or 7)\n",
        "\n",
        "print(f\"ü§ñ Training final LDA model with {num_topics} topics...\\n\")\n",
        "\n",
        "lda_model = LdaModel(\n",
        "    corpus=corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    random_state=42,\n",
        "    passes=20,  # More passes for better final model\n",
        "    alpha='auto',\n",
        "    eta='auto'\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Final model training complete!\\n\")\n",
        "print(\"üéØ YOUR DISCOVERED TOPICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    words = lda_model.show_topic(idx, 10)\n",
        "    word_list = [word for word, prob in words]\n",
        "    print(f\"\\nTopic {idx}: {', '.join(word_list)}\")\n",
        "    print(f\"Your interpretation/label: _____________________\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v29E-7ciUboE"
      },
      "outputs": [],
      "source": [
        "# Visualize your topics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(num_topics, 1, figsize=(12, 4*num_topics))\n",
        "\n",
        "if num_topics == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx in range(num_topics):\n",
        "    words_weights = lda_model.show_topic(idx, 10)\n",
        "    words = [word for word, weight in words_weights]\n",
        "    weights = [weight for word, weight in words_weights]\n",
        "\n",
        "    axes[idx].barh(range(len(words)), weights, color='skyblue')\n",
        "    axes[idx].set_yticks(range(len(words)))\n",
        "    axes[idx].set_yticklabels(words)\n",
        "    axes[idx].set_xlabel('Weight')\n",
        "    axes[idx].set_title(f'Topic {idx} - [Add your label here]')\n",
        "    axes[idx].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Topic visualizations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-oE89jcUboF"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 5: Validating Topic Assignments\n",
        "*Human vs. Algorithm*\n",
        "\n",
        "Let's see which topics the model assigns to documents and check if they make sense:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fCIUme4UboF"
      },
      "outputs": [],
      "source": [
        "# Get dominant topic for each document\n",
        "def get_document_topics(lda_model, corpus):\n",
        "    \"\"\"\n",
        "    Get dominant topic assignment for each document\n",
        "    \"\"\"\n",
        "    topic_assignments = []\n",
        "\n",
        "    for doc in corpus:\n",
        "        topic_dist = lda_model.get_document_topics(doc)\n",
        "        if topic_dist:  # Check if not empty\n",
        "            dominant_topic = max(topic_dist, key=lambda x: x[1])\n",
        "            topic_assignments.append({\n",
        "                'topic_num': dominant_topic[0],\n",
        "                'topic_prob': round(dominant_topic[1], 3)\n",
        "            })\n",
        "        else:\n",
        "            topic_assignments.append({\n",
        "                'topic_num': -1,\n",
        "                'topic_prob': 0.0\n",
        "            })\n",
        "\n",
        "    return topic_assignments\n",
        "\n",
        "# Get topic assignments\n",
        "topic_info = get_document_topics(lda_model, corpus)\n",
        "df['dominant_topic'] = [t['topic_num'] for t in topic_info]\n",
        "df['topic_probability'] = [t['topic_prob'] for t in topic_info]\n",
        "\n",
        "print(\"‚úÖ Topic assignments complete!\")\n",
        "print(f\"\\nTopic distribution across documents:\")\n",
        "print(df['dominant_topic'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gTy05xzUboG"
      },
      "outputs": [],
      "source": [
        "# Sample documents from each topic for validation\n",
        "print(\"üîç DOCUMENT-TOPIC VALIDATION CHECK\")\n",
        "print(\"=\" * 70)\n",
        "print(\"For each topic, read sample documents and assess if the assignment makes sense:\\n\")\n",
        "\n",
        "for topic_num in range(num_topics):\n",
        "    print(f\"\\nüìå TOPIC {topic_num}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Get top words for this topic\n",
        "    topic_words = lda_model.show_topic(topic_num, 8)\n",
        "    word_list = [word for word, prob in topic_words]\n",
        "    print(f\"Keywords: {', '.join(word_list)}\")\n",
        "\n",
        "    # Get sample documents from this topic\n",
        "    topic_docs = df[df['dominant_topic'] == topic_num]\n",
        "\n",
        "    if len(topic_docs) == 0:\n",
        "        print(\"No documents assigned to this topic.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nDocuments in this topic: {len(topic_docs)}\")\n",
        "    print(f\"\\nSample documents (read and assess if topic assignment makes sense):\\n\")\n",
        "\n",
        "    for i, (idx, row) in enumerate(topic_docs.head(3).iterrows(), 1):\n",
        "        print(f\"  {i}. {row[text_column][:150]}...\")\n",
        "        print(f\"     Probability: {row['topic_probability']:.3f}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP89hP5WUboG"
      },
      "source": [
        "### üìù Model Assessment (2 paragraphs)\n",
        "\n",
        "**Paragraph 1 - Prediction vs. Reality:**\n",
        "\n",
        "*(How did your topic modeling results compare to your specific predictions from HW4-1 Step 5? What topics emerged that you hadn't expected? Were you surprised by what the algorithm found?)*\n",
        "\n",
        "\n",
        "**Paragraph 2 - Model Validation:**\n",
        "\n",
        "*(Look at the sample documents above for each topic. Do the assigned topics make sense when you read the full text? Where would you disagree with the model? What does this tell you about the algorithm's interpretation vs. human understanding?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvgbJ2WrUboG"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 6: Integration Across All Three Methods\n",
        "*Connecting Term Frequency, Sentiment, and Topics*\n",
        "\n",
        "Now let's explore how all three analytical methods work together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEkJe8myUboH"
      },
      "outputs": [],
      "source": [
        "# Analyze sentiment patterns within each topic\n",
        "print(\"üìä SENTIMENT BY TOPIC ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for topic_num in range(num_topics):\n",
        "    topic_docs = df[df['dominant_topic'] == topic_num]\n",
        "\n",
        "    if len(topic_docs) == 0:\n",
        "        continue\n",
        "\n",
        "    avg_sentiment = topic_docs['sentiment_score'].mean()\n",
        "\n",
        "    # Get topic keywords\n",
        "    topic_words = lda_model.show_topic(topic_num, 5)\n",
        "    word_list = [word for word, prob in topic_words]\n",
        "\n",
        "    print(f\"\\nTopic {topic_num}: {', '.join(word_list)}\")\n",
        "    print(f\"  Documents: {len(topic_docs)}\")\n",
        "    print(f\"  Average sentiment: {avg_sentiment:.3f}\")\n",
        "    print(f\"  Sentiment range: {topic_docs['sentiment_score'].min():.3f} to {topic_docs['sentiment_score'].max():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkreu0HGUboH"
      },
      "outputs": [],
      "source": [
        "# Visualize sentiment distribution by topic\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Create box plot of sentiment scores by topic\n",
        "topic_sentiment_data = [df[df['dominant_topic'] == i]['sentiment_score'].values\n",
        "                        for i in range(num_topics)]\n",
        "\n",
        "ax.boxplot(topic_sentiment_data, labels=[f'Topic {i}' for i in range(num_topics)])\n",
        "ax.set_xlabel('Topic')\n",
        "ax.set_ylabel('Sentiment Score')\n",
        "ax.set_title('Sentiment Distribution by Topic')\n",
        "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='Neutral')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Sentiment by topic visualization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhBpjzH7UboH"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive summary\n",
        "print(\"üìã COMPLETE ANALYTICAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nDataset: {len(df)} documents\")\n",
        "print(f\"\\n1Ô∏è‚É£ TERM FREQUENCY (from HW4-1):\")\n",
        "print(f\"   Vocabulary size: {len(dictionary)} unique words\")\n",
        "print(f\"   [Your top words from HW4-1]\")\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£ SENTIMENT ANALYSIS (from HW4-1):\")\n",
        "print(f\"   Average sentiment: {df['sentiment_score'].mean():.3f}\")\n",
        "print(f\"   Positive: {len(df[df['sentiment_score'] > 0.1])} ({len(df[df['sentiment_score'] > 0.1])/len(df)*100:.1f}%)\")\n",
        "print(f\"   Neutral: {len(df[(df['sentiment_score'] >= -0.1) & (df['sentiment_score'] <= 0.1)])} ({len(df[(df['sentiment_score'] >= -0.1) & (df['sentiment_score'] <= 0.1)])/len(df)*100:.1f}%)\")\n",
        "print(f\"   Negative: {len(df[df['sentiment_score'] < -0.1])} ({len(df[df['sentiment_score'] < -0.1])/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£ TOPIC MODELING (HW4-2):\")\n",
        "print(f\"   Number of topics: {num_topics}\")\n",
        "for topic_num in range(num_topics):\n",
        "    topic_docs = df[df['dominant_topic'] == topic_num]\n",
        "    if len(topic_docs) > 0:\n",
        "        topic_words = lda_model.show_topic(topic_num, 3)\n",
        "        word_list = [word for word, prob in topic_words]\n",
        "        print(f\"   Topic {topic_num} ({len(topic_docs)} docs, avg sentiment {topic_docs['sentiment_score'].mean():.2f}): {', '.join(word_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX2CGFSUUboI"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 7: Final Reflection and Integration\n",
        "*From Assumptions to Data-Driven Insights*\n",
        "\n",
        "This is the heart of the assignment. Reflect deeply on your complete analytical journey."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nYy483TUboJ"
      },
      "source": [
        "### üìù Part A: Method and Tool Assessment (2 paragraphs)\n",
        "\n",
        "**Paragraph 1 - Evolution of Understanding:**\n",
        "\n",
        "*(How did your understanding change from your initial predictions through term frequency, sentiment analysis, and topic modeling? Where were you most wrong, and what did those surprises teach you? What questions emerged that you hadn't anticipated?)*\n",
        "\n",
        "\n",
        "**Paragraph 2 - Analytical Methods:**\n",
        "\n",
        "*(Which analytical method (term frequency, sentiment analysis, or topic modeling) was most revealing for your dataset? What did you learn about what these tools can and cannot tell you? How do they complement each other?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeSFoZDtUboJ"
      },
      "source": [
        "### üìù Part B: Humanities Perspective (1-2 paragraphs)\n",
        "\n",
        "**Computational vs. Traditional Analysis:**\n",
        "\n",
        "*(How does this data-driven approach compare to how you might traditionally analyze texts in humanities courses? Consider how this process helps you avoid the trap of cherry-picking evidence to support predetermined conclusions. What are the advantages and limitations of automated analysis? What role does human interpretation still play?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tssq6ygDUboK"
      },
      "source": [
        "### üìù Part C: Personal Learning (1 paragraph)\n",
        "\n",
        "**Your Analytical Growth:**\n",
        "\n",
        "*(What did this complete process (HW4-1 + HW4-2) teach you about forming and testing hypotheses? How has your relationship to evidence and conclusions evolved? What will you take from this experience into future research or analysis?)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D33z-CDNUboK"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 8: Save Your Work and Prepare Deliverables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvW6_LUuUboL"
      },
      "outputs": [],
      "source": [
        "# Save your final integrated dataset\n",
        "output_filename = 'LASTNAME_HW4-2_integrated_data.csv'  # Replace LASTNAME\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"‚úÖ Integrated dataset saved as: {output_filename}\")\n",
        "print(f\"\\nFinal dataset summary:\")\n",
        "print(f\"- Total entries: {len(df)}\")\n",
        "print(f\"- Columns: {df.columns.tolist()}\")\n",
        "print(f\"- Number of topics: {num_topics}\")\n",
        "print(f\"- Average sentiment: {df['sentiment_score'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFentCQUboL"
      },
      "source": [
        "### üìã Submission Checklist\n",
        "\n",
        "Before submitting, make sure you have:\n",
        "\n",
        "- [ ] **Reviewed your HW4-1 predictions** and copied them to Step 1\n",
        "- [ ] **Added custom domain-specific stopwords** appropriate for your dataset\n",
        "- [ ] **Experimented with different numbers of topics** and chose the most interpretable\n",
        "- [ ] **Labeled each discovered topic** with meaningful names\n",
        "- [ ] **Validated topic assignments** by reading sample documents\n",
        "- [ ] **Completed all reflection paragraphs** with thoughtful, specific responses\n",
        "- [ ] **Integrated findings** across term frequency, sentiment, and topic modeling\n",
        "- [ ] **Named your files correctly**: `LASTNAME_HW4-2_F25.ipynb`\n",
        "\n",
        "### üì§ What to Submit:\n",
        "\n",
        "Submit to your **Pre-Project Practice Files** dropbox:\n",
        "1. **Jupyter Notebook** (`LASTNAME_HW4-2_F25.ipynb`) with all code and markdown responses\n",
        "2. **Any new visualization PNG files** (if not embedded in notebook)\n",
        "\n",
        "**Due Date**: October 13th\n",
        "\n",
        "---\n",
        "\n",
        "## Grading Philosophy\n",
        "\n",
        "This assignment prioritizes **earned insight over clean code**. You'll be evaluated on:\n",
        "\n",
        "- **Depth of reflection** and willingness to be surprised by your data\n",
        "- **Evolution of thinking** from initial predictions through final analysis\n",
        "- **Critical assessment** of both your results and your analytical tools\n",
        "- **Integration** of technical analysis with humanistic interpretation\n",
        "\n",
        "Technical execution matters, but **intellectual growth and genuine insight are the primary goals**. The best submissions will show students whose understanding deepened and changed through engagement with their data.\n",
        "\n",
        "**Remember**: Being \"wrong\" in your predictions is not failure‚Äîit's evidence of genuine learning. The goal is not to confirm what you already thought, but to discover what the data reveals.\n",
        "\n",
        "---\n",
        "\n",
        "## Congratulations! üéâ\n",
        "\n",
        "You've completed a comprehensive text analysis journey, moving from raw data through term frequency, sentiment analysis, and topic modeling to earned insights. This is the kind of data-driven analytical thinking that defines digital humanities research.\n",
        "\n",
        "The skills you've practiced‚Äîforming hypotheses, testing them systematically, being surprised by results, and integrating multiple analytical methods‚Äîwill serve you well in any future research or analysis work."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}